{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790767a2-2e8d-4239-a08c-8a67cc040dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d46cda-5535-4814-82a2-13b78c568461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98bc2210-83de-4114-b970-d1bb64f0f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "block_size = 128\n",
    "max_iters = 2400\n",
    "eval_interval = 300\n",
    "learning_rate = 5e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdc1ed8-357a-4a31-b009-a3a8b7376e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gone_with_the_wind.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text = enc.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b18ddd9-cc4a-4e73-aebc-c1ca71f708cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(text, dtype=torch.long, device=device)\n",
    "n = int(0.8 * len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a715b61a-660f-4790-9903-591afb32806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "381a5321-5f64-45e0-aca1-1b09735bd275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    n_vocab: int\n",
    "    d_model: int\n",
    "    n_block: int\n",
    "    n_head: int\n",
    "    n_layer: int\n",
    "    d_inner: int\n",
    "    dropout: float\n",
    "    emb_dropout: float\n",
    "    bias: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.d_k = self.d_v = self.d_model // self.n_head\n",
    "        self.n_embd = self.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed522c62-6ec2-43fe-872a-bce2a50900c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.qkv = nn.Linear(config.d_model, 3*config.d_model, bias=config.bias)\n",
    "        self.proj = nn.Linear(config.d_model, config.d_model, bias=config.bias)\n",
    "        self.proj_drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.dropout_p = config.dropout\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.d_model = config.d_model\n",
    "        self.d_k = config.d_k\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        q,k,v = self.qkv(x).split(self.d_model, dim=2)\n",
    "        q = q.view(B, T, self.n_head, self.d_k).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.d_k).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        y = nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout_p if self.train else 0, is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        y = self.proj_drop(self.proj(y))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model,  config.d_inner, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(config.d_inner, config.d_model, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.mlp = MLP(config)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.word_embeddings = nn.Embedding(config.n_vocab, config.d_model)\n",
    "        self.position_embeddings = nn.Embedding(config.n_block, config.d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layer)])\n",
    "        self.layernorm = nn.LayerNorm(config.d_model)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.n_vocab, bias=False)\n",
    "        \n",
    "        self.lm_head.weight = self.word_embeddings.weight\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        tok_emb = self.word_embeddings(idx)\n",
    "        pos_emb = self.position_embeddings(torch.arange(idx.shape[1], device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:,[-1],:])\n",
    "            loss = None\n",
    "            \n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=-1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86141c27-f025-48be-846a-d15c2d14e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    n_vocab=enc.n_vocab,\n",
    "    d_model=128,\n",
    "    n_block=block_size,\n",
    "    n_head=4,\n",
    "    n_layer=6,\n",
    "    d_inner=512,\n",
    "    dropout=0.2,\n",
    "    emb_dropout=0.0,\n",
    "    bias=True\n",
    ")\n",
    "model = Transformer(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1054e8d4-9d7a-41d9-8101-3bc5c4009b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41c8d16c-ba8b-44fe-9781-fba85559bdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2400 [00:05<3:30:53,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 10.7486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 301/2400 [06:01<1:30:04,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 5.8670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 601/2400 [11:59<1:16:46,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 5.1771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 901/2400 [17:56<1:04:14,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.8755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1201/2400 [23:55<51:53,  2.60s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.6956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1501/2400 [29:56<39:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.6197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1801/2400 [35:54<25:39,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.5403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 2101/2400 [41:53<12:52,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.4818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2400/2400 [47:46<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "for iter in tqdm.tqdm(range(max_iters)):\n",
    "    x, y = get_batch('train')\n",
    "    logits, loss = model(x, y)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if iter % eval_interval == 0:\n",
    "        times = 10\n",
    "        total_loss = 0\n",
    "        for _ in range(times):\n",
    "            x, y = get_batch('val')\n",
    "            logits, loss = model(x, y)\n",
    "            total_loss += loss.item()\n",
    "        print(f'Validation loss: {total_loss / times:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b348fa64-ab0c-40ca-99e3-5bc01334caba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.48477029800415\n",
      "She smiled, and the world was hers. She was not to have a child and she had not know.\n",
      "　“I’m afraid of the world,” he said. “I’ll think I’ve got a silly. I’m not sorry to have a man.”\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch('val')\n",
    "logits, loss = model(x, y)\n",
    "print(f'Validation loss: {loss.item()}')\n",
    "\n",
    "ctx = 'She smiled, and the world was hers.'\n",
    "ctx = enc.encode(ctx)\n",
    "ctx = torch.tensor(ctx, dtype=torch.long, device=device).unsqueeze(0)\n",
    "ctx = model.generate(ctx, 60, 0.4)\n",
    "ctx = ctx.squeeze(0).tolist()\n",
    "ctx = enc.decode(ctx)\n",
    "print(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89f938f4-b514-4202-b5eb-d3931905c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-design-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
